Packet Classification using Tuple Space Search 
Abstract 
V. Srinivasan* S. Suri+ G. Vargheset 
cheenu@ccrc.wustl.edu, suri&s.wustl.edu, vargheseQccrc.wustl.edu 
Department of Cornputer Science, Washington University in $t. Louis 
Routers must perform packet classification at high speeds 
to efficiently implement functions such as firewalls and 
QoS routing. Packet classification requires matching each 
packet against a database of filters (or rules), and for￾warding the packet according to the highest priority fil￾ter. Existing filter schemes with fast lookup time do 
not scale to large filter databases. Other more scal￾able schemes work for Z-dimensional filters, but their 
lookup times degrade quickly with each additional di￾mension. While there exist good hardware solutions, our 
new schemes are geared towards software implementa￾tion. 
We introduce a generic packet classification algorithm, 
called Tuple Space Search (TSS). Because real databases 
typically use only a small number of distinct field lengths, 
by mapping filters to tuples even a simple linear search 
of the tuple space can provide significant speedup over 
naive linear search over the filters. Each tuple is main￾tained as a hash table that can be searched in one mem￾ory access. We then introduce techniques for further 
refining the search of the tuple space, and demonstrate 
their effectiveness on some firewall databases. For exam￾ple, a real database of 278 filters had a tuple space of 
41 which our algorithm prunes to 11 tuples. Even as we 
increased the filter database size from 1Ii to 1OOK (us￾ing a random two-dimensional filter generation model), 
the number of tuples grew from 53 to only 186, and the 
pruned tuples only grew from 1 to 4. Our Pruned Tuple 
Space search is also the only scheme known to US that 
allows fast updates and fast search times. We also show 
a lower bound on the general tuple space search prob￾lem, and describe an optimal algorithm, called Rectangle 
Search, for two-dimensional filters. 
*Research supported in part by NSF Grant NCR-9628145. 
‘Research supported in part by NSF Grant 9813723 
*Research supported in part by NSF Grant NCR 9813733. 
Permission to make digital or hard copies of all or pert of this work for 
personal or classroom use is granted without fee provided that 
copies are not made or distributed for profit or commercial advan￾tage and that copies bear this notice and the full citation on the first page. 
To copy otherwise, to republish, to post on servers or to 
redistribute to lists, requires prior specific permission and/or e fee. 
SIGCOMM ‘99 8/99 Cambridge. MA, USA 
0 1999 ACM l-58113-1 35-8/99/0008...$5.00 
1 Introduction 
As the Internet begins to be used for commercial appli￾cations, service providers would like routers to provide 
“service differentiation”. Traditional routers do not pro￾vide service differentiation because they treat all traffic 
going to the same Internet dest.ination address identi￾cally. Routers with a packet classification [8, 131 capabil￾ity, however, can distinguish traffic based on destination, 
source, and application type. Such classification allows 
various forms of service differentiation: blocking traffic 
sent by insecure sites (firewalls), preferential treatment 
for premium traffic (resource reservation), and routing 
based on traffic type and source (QoS routing). 
While more general applications like resource reser￾vation 121) and QoS routing are likely to be part of fu￾ture routers, many routers today implement firewalls [3] 
at trust boundaries, such as the entry and exit points 
of a corporate network. A firewall database consists 
of a series of packet filters (or rules based on packet 
header fields) that implement security policies. Despite 
the progress made in the last year on solutions to the 
packet classification problem [8, 131, existing firewall soft￾ware is still slow. 
While the general solution in [S] can handle thou￾sands of filters at very high speeds, it is geared towards 
hardware and uses hardware parallelism and high speed 
memories. The solutions in [13] have a high worst case 
figure for the general packet filter problem. Thus there 
is room for further research especially in the area of so& 
Mare packet classification. Existing solutions are also op￾timized for the case when updates are infrequent. How￾ever, many firewall vendors now offer stateful filters [4]. 
For example, the sending of a UDP request may trigger 
the addition of a filter addition that allows the response 
to flow past the firewall. This may require filter insertion 
in the order of microseconds. Other applications that 
may require fast filter updates include resource reserva￾tion protocols like RSVP [2]. 
Thus faster software packet classification with fast 
update times can benefit screening routers and many 
commercial firewall software packages. It can also be 
useful for other applications of packet classification im￾plemented in say endnodes, which are unlikely to use 
FPGA or ASIC based solutions. 
135 
A general filter consists of arbitrary prefix or range 
specifications on the destination, source, protocol, port 
number and possibly other fields. There is evidence that 
the general filter problem is a hard problem [8, 131 and 
requires either memory of NIi or time of O(N), where 
N is the number of filters and Ii is the number of dimen￾sions. We confirm this growing body of evidence in our 
paper with some new lower bounds in a hashing model, 
which complement the earlier lower bounds on multidi￾mensional range matching quoted in [S, 131. 
The lower bounds indicate that to do better one has 
to exploit the semantics of actual databases. Since tie￾wall databases are commonly used, we decided to exam￾ine act,ual firewall databases to see if there were some 
regularities we could exploit. On examination we found 
that there were only a few combinations of field lengths 
used in firewall filters. Intuitively, this follows because 
most address prefixes are based on Class C (24 bit) and 
Class B (16 bits) prefixes. Similarly port fields are typ￾ically either fuUy specified port numbers (e.g., port. 23), 
the wildcard range (*), or the single range (> 1024 or 
5 1023).’ 
This motivated us to examine what we call Tuple 
Search. In its simplest form, Tuple Search examines the 
space of tuples in a filter database, where a t.uple is a 
combination of field lengths. Each check for a tuple can 
be efficiently done by hashing. Next, we develop some 
additional heuristics for speeding up Tuple Search, and 
use them to construct efficient and practical implcmen￾tations, some of which have fast update times. We also 
develop the theory behind Tuple Search. We show that 
the general filter case is indeed hard in the Tuple Search 
paradigm as well, confirming the intuition in [8, 131. For 
the special case of two-dimensional filters, we describe 
an optimal Tuple Search scheme called Rectangle Search, 
whose performance is comparable to the two-dimensional 
algorithms presented in [8, 131. 
This paper is organized as follows. We provide back￾ground on firewalls in Section 2, and formally describe 
the packet classification problem in Section 3. We sur￾vey related work in Section 4. We start our discussion 
of tuple space search in Section 5 with the simplest tu￾ple search. In Section 6, we describe a simple heuris￾tic variant of the basic tuple search called Tuple Prun￾ing, that has fast search and update times. While Tu￾ple Pruning appears to be our most practical algorithm 
(based on our experience with real databases), its worst 
case case search time for arbitrary databases can be bad. 
In Section 7, we show how to improve the worst case 
search time of tuple search using markers and precom￾putation. In Section 8, we also describe an optimal algo￾rithm for 2-dimensional filters. Next, we describe lower 
bounds on the general tuple search problem in Section 9. 
In Section 10 we describe another balancing heuristic 
for comput.ing a good probe sequence for general tuple 
spaces while exploiting markers and precomputation. We 
conclude in Section 11. 
‘Because BSD IJNIX reserves ports 0 to 1023 for local use only 
by root., these ports are only used by servers, not clients. Other 
operating systems have followed this custom. This allows packets 
sent by serwxs to be distinguished from packets sent by clients. 
Filters for X servers are another non-trivial example of a port 
range (e.g., 60000-61000) but these are less common. 
2 A Brief Introduction to Firewalls 
While the t.echniques in our paper are applicable to any 
application that requires packet classification, we pro￾vide some background on tiewalls. Firewalls provide a 
concrete application of packet classification where fast 
software implementations are currently desired. 
Firewalls are implemented using various combinations 
of two basic techniques [4]: packet filtering and applica￾tion level gatezuays [also known as proxy services). In 
packet filtering, a so-called screening router (also known 
as a choke router) sit.s between the external and intcr￾nal worlds, and allows or blocks certain types of packets. 
Unlike conventional routers, screening routers make their 
decision based on Layer 3 headers as well as Layer 4 and 
even application headers. For example, this flexibility 
allows a screening router to block Telnet (by blocking 
packets sent to TCP port 23) or to disallow incoming 
TCP connections from some ports (by inspecting TCP 
flags). 
Application level gateways are specialized server pro￾grams that run on firewall hosts that take user requests 
(e.g., for Telnet and FTP) and forward them according 
to the site policy. An advantage of proxy services is that 
they are specialized for each service and can thus imple￾ment more sophisticated policies than a screening router 
by itself. For example, an FTP proxy can allow some 
users to import files only from some sites. Because they 
understand t.he protocol, Proxy Services can also pro￾vide more intelligent logging (useful to detect the onset 
of an attack). On the other hand, one has t,o construct a 
proxy service for each possible service one supports and 
keep it current as implementations change; proxy ser￾vices usually require some modifications to clients and 
servers; and proxies don’t work with all services [4]. Fur￾t,her, proxy services are only effective with one or more 
screening routers that restrict communication with the 
host that, implements the proxy service. 
Thus in practice, various combinations of the two 
techniques are used. For example, Telnet and SMTP 
are handled well using packet filtering. Web services 
seem best handled using proxies [4] because web prox￾ies are easily available and can improve performance by 
caching. Passive mode FTP can be handled using packet 
filtering, but. normal model FTP can use port numbers 
higher than 1023, which can allow access to other ser￾vices. Thus normal mode FTP is often handled using a 
proxy. 
The most important point to gather from our brief 
introduction to firewalls is that software packet filtering 
routers are an important part of real firewall conjigura￾tions today. There are clearly other important compo￾nents such as proxy and logging services, but packet fil￾tering is important by itself. Our paper is about improv￾ing the performance of software packet classification and 
allowing fast updates. This in turn can allow fast soft￾ware implementations of screening routers, but can also 
be used for other packet classifcation applications such 
as resource reservation. 
136 
3 Problem Statement 4 Related Work 
Suppose there are Ii header fields in each packet that 
are relevant to filtering. Then, each filter F is a K￾tuple (F[l], F[2], . . . . F[k]), where each F[i] is either 
a variable length prefix bit string or a range. The most 
common fields are the IP destination address (32 bits); IP 
source address (32 bits), protocol type (8 bits), and port 
numbers (16 bits) of destination and source applications, 
and protocol flags. Since the number of distinct protocol 
flags, such as TCP ack, are limited, we can combine them 
into the protocol field itself. (TCP flags are important for 
packet filtering because the first packet in a connection 
does not have the ACK bit set. while the others do; this 
allows a simple filter to block TCP connections initiated 
from the outside while allowing responses to internally 
initiated connections.) 
The filter F = (128.112.*, *, TCP, 23, *), as an ex￾ample, specifies a rule for traffic flow addressed to subnet 
128.112 using TCP destination port 23, which is used for 
incoming Telnet; a firewall database may disallow Telnet. 
into its network. 
A filter dat,abase consists of N filters FI, F2, . . . , FN. 
Each filter F is an array of Ii distinct fields, where F[i] 
is a specification on the i-th field. We often refer to the 
i-th field as the i-th dimension. Each field i in a fil￾ter is allowed three kinds of matches: exact, prefix, and 
range. Each filter F, has an associated directive: for ex￾ample, firewall database could specify whether to accept 
or block a packet. Each filter also has an associated cost; 
in firewall databases, filters are linearly ranked, and the 
position of a filter is used as its cost. 
We say that a packet P matches filter F if for all 
packet fields i, P[i] matches F[i]. The packet classifica￾tion problem is to find the lowest cost filter matching a 
given packet P. 
While our examples only use simple fields in the IP 
and TCP headers, so called “third-generation” filtering 
products [4] are emerging that can use applicat.ion header 
fields, and also other parameters such as the input link 
and time-of-day. We note that our techniques apply to 
the use of other fields as well. For the particular case of 
filters that depend on the input link (useful for prevent￾ing forged source addresses), the simplest technique is to 
use a separate database per input link. 
While the cost of inserting a rule may seem less im￾portant than search, this is not true for dyncdmic or state￾fu/ packet filters. This capability is useful, for exam￾ple, for handling UDP traffic. Because UDP headers do 
not contain an ACE; bit that can be used to determine 
whether a packet is the bellwether packet of a connec￾tion, the screening router cannot tell the difference be￾tween the first packet sent from the outside t.o an internal 
server (which it may want to block) and a response sent 
to a UDP request to an internal client (which it may 
want to pass). The solution used in some products is 
to have the outgoing request packet dynamically trigger 
the insertion of a filter (that has addresses and ports that 
match the request) that allows the inbound response to 
be passed. This requires very fast. update times. 
For several years, packet filters have been used for de￾multiplexing of incoming packets directly to user pro￾cesses [lo, 1, 61. The first packet filtering scheme (to our 
knowledge) that avoids a linear search through the set 
of filters is PathFinder [l]. However, PathFinder only 
allows filters that have wild card fields at the end of 
the filter (for instance, (II, S, *, *, *) is allowed, but not, 
(II, *, Prot, *, SrcPorf)). For such a restricted case, all 
filters can be merged int.o a generalized trie (with hash 
tables replacing array nodes) and filter lookup can be 
done in time proportional to the number of packet fields. 
DPF [6] uses the PathFinder idea of merging filters into 
a trie but adds the idea of using dynamic code genera￾tion for extra performance. However, it is unclear how 
to handle intermixed wildcards and specified fields, such 
as (0, *, Prot, *, SrcPorl), using these schemes. 
Because our problem allows more general filters, the 
PathFinder idea of using a trie does not work. There 
does exist a simple t.rie scheme to perform a lookup in 
time O(K), where I( is the number of packet fields. The 
basic idea is to consider a trie as a deterministic state 
machine and to realize that a general filter can be recog￾nized by a non-deterministic automaton (NDA). Such a 
NDA can be converted to a trie but with an exponential 
blowup in the storage cost,. Such schemes are described 
(using DAGs instead of trees) in [5,9]. To the best of our 
knowledge, such schemes require O(NI’) storage where 
K is the number of packet fields and N is the number 
of filters. Thus such schemes are not scalable for large 
databases. By contrast, our new schemes require only 
O(NK) storage. 
Linear Search and Caching: Another simple way to 
solve the filter problem is to do a linear search of the set 
of filters against a header and then to cache the result 
of the search keyed against the whole header. There are 
two problems w&h this scheme. First, the cache hit rate 
of full IP addresses in the backbones is typically at most 
80-90 percent [12, II]. The poor cache hit rate is caused 
in part by web traffic, where each flow consists of just a 
few packets; if a web session sends just 5 packets to the 
same address, then the cache hit rate is 80 percent. Since 
caching full headers takes a lot more memory, the cache 
hit rate should be even worse for the packet classification 
problem (for t,he same amount, of cache memory). Sec￾ond, Amdahl’s Law shows that even with a 90 percent 
rate cache, a slow linear search of the filter space will 
result in poor performance. For example, suppose that a 
search of the cache costs 100 nsec (one memory access) 
and linear search of 10,000 filters costs 1000,000 nsec = 
1 msec (one memory access per filter). Then the average 
search time with a cache hit rate of 90 percent is still 0.1 
msec, which is rather slow. 
Grid-of-Tries and Crossproducting: One of the so￾lutions presented last year for the filter matching prob￾lem [13] decomposes the multidimensional problem int,o 
several 2-dimensional planes, and uses a data structure, 
grid-of-tries, to solve the 2dimensional problem. This 
solution does not scale well as the number of required 
137 
planes increase. For practical firewall databases, it re￾quires up to 8 planes, with each plane requiring upto 8 
memory accesses. This results in a worst case of 64 mem￾ory accesses. Another limitation of this scheme is t,hat it 
does not allow arbitrary ranges. 
-4 second solution in [13] is called crossproducting. In 
this scheme, a longest matching lookup or a range lookup 
is first performed in each of the dimensions. The results 
are then concatenated to form a crossproduct, which is 
then mapped to a best matching filter. While crosspro￾ducting has good lookup times, it either requires O(N”) 
memory or does not provide det,erministic search time 
guarantees. 
Hardware solutions: Hardware solutions can poten￾tially use parallelism to gain lookup speed. For exact 
matches, this is done using Content Addressable Memo￾ries (CAMS) in which every memory location, in paral￾lel, compares the input key value to the contents of that 
memory location. This can be generalized if the CAMS 
allow certain field positions to be masked out. How￾ever, it is difficult to manufacture CAMS with the width 
required to solve filter problems. Second, hardware solu￾tions run the risk of being made obsolete, in a few years, 
by software technology running on faster processors and 
memory. 
A clever hardware approach is presented in [8]. While 
this scheme is optimized for hardware, it can work quite’ 
well in software for moderate sized databases. It involves 
reading a bitmap of size N bits for each of IC dimensions. 
The scheme starts by computing the best matching pre￾fix (or closest enclosing range) for each dimension. With 
each such prefix (or range) P is associated an N bit vec￾tor BP. Bit i of Bp is set if P is compatible with the 
i-th filter in the database. Finally, the intersection of the 
li bitmaps is computed, and the filter corresponding to 
t,he first bit set in the intersect,ion is returned as the re￾sult. If we consider 5-dimensional filters and N = 1000, 
t.his involves reading 5000 bits. With a cache line size of 
32 bytes, this is only about 19 memory accesses to main 
memory. However, in its simplest form, the memory re￾quirement for this scheme is O(N’). Thus it does not 
scale well to large databases. The update t.imes are also 
slow because of t.he potential need to update all bitmaps 
when a new filter is added. Some techniques presented in 
[8] allow these bitmaps to be compressed to O(Nlog N), 
but these schemes involve reading N log N bits per di￾mension during search, instead of N. 
5 Tuple Space Search 
Our scheme is motivated by the observation that while 
filter dat.abases contain many different prefixes or ranges, 
the number of distinct prefix lengths tends to be small. 
Thus, the number of distinct combinations of prefix lengths 
is also small. This observation seems to be validated by 
our empirical study of some industrial firewall databases. 
We can define a tuple for each combination of field length, 
and call the resulting set tuple space. Since each tuple 
has a known set of bits in each field, by concatenating 
these bits in order we can create a hash key, which can 
then be used to map filters of that tuple into a hash table. 
‘Suppose we have a filter database FD with N filters, and 
these filters result in m distinct tuples. Since m tends to 
be much smaller than N in practice, even a linear search 
through the tuple set is likely to greatly outperform the 
linear search through the filter database. Starting with 
this simple observation, we then develop several opti￾mizations that, reduce the search cost further. With this 
motivation, we first define the notion of tuple space more 
formally. 
5.1 Defining Tuple Space 
Consider a filter database FD that contains N filters, 
each specifying li fields. We will call these K-dimensional 
filters. While our results are general, we will explicitly 
consider 5-dimensional filters in our examples and ex￾periments, whose fields are IP source, IP destination, 
protocol type, source port number, and the destination 
port number. IP source and destination prefixes have at 
most 32 bits; the protocol is specified by 8 bits; and the 
port numbers are 16 bit addresses. 
A tuple T is vector of K lengths. Thus, for example, 
[8, 16, 8, 0, 161 is a 5-dimensional tuple, whose IP source 
field is a 8-bit prefix, IP destination field is a 16-bit prefix, 
and so on. We say that a filter F belongs or maps to 
tuple T if the ath field of F is specified to exactly T[i] 
bits. For example, considering 2-dimensional filters, both 
FI = (Ol*, ill*) and Fz = (ll*, OlO*) map to the tuple 
[2,31. 
Filters always specify IP source or destination ad￾dresses using prefixes, so the number of bits specified is 
clear. The port numbers, however, are often specified us￾ing ranges, and the number of bits specified is not clear. 
To get around this, we define the length of a port range 
to be its nesting level. For instance, the full port num￾ber range [0,65535] has nesting level and length 0. The 
ranges [0,1023] and [1024,65535] are considered to be 
nesting level 1, and so on. If we had additional ranges 
[30000,34000] and [31000,32000], then the former will 
have nesting level 2 and the latter level 3. (We assume 
that port number ranges specified in a database are non￾overlapping.) 
While the nesting level of a range helps define the 
tuple (or hash table) it will be placed in, we also need a 
key to identify the filter within the hash table. To this 
end, we use a Rangeld, which is a unique id given to each 
range in any particular nesting level. So the full range 
always has the id 0. The two ranges at depth 1, namely, 
5 1023 and > 1034, receive the ids 0 and 1 respectively. 
Suppose we had ranges 200.. ,333, 32000.. .34230 and 
60000.. .65500 at level 2, then they would be given ids 
0,l and 2 respectively. With range ids, we can now map 
any 5-dimensional filter to a S-dimensional tuple. To 
underst,and how the RangeId works, let us draw an anal￾ogy between prefixes and ranges. An address D can have 
some m prefixes PI . . . P,,, in the database that match 
it, such that P,-1 is a prefix of Pi. In the case of a 
range match, each of these prefix lengths correspond to 
a nesting depth. 
Notice that. a given port in a packet header can map 
to a different ID at each nesting level. For example, with 
the above ranges, a port number 33000 will map on to 
three RangeId values, one for each nesting depth: Id 0 
13x 
for nesting depth 0: Id 1. for nesting depth 1, and Id 1 for 
nesting dept.h 2. Thus a port number field in a packet 
header must be translated to its corresponding Rangeld 
values before tuple starch is performed. In summary, 
the nesting level is used to determine the tuple, and the 
RangeId for each nesting level is used to form the hash 
key. 
5.2 Searching the Tuple Space 
All filters that map to a particular tuple have the same 
mask: some number of bits in the IP source and des￾tination fields, eit.her a wild card in the protocol field 
or a specific protocol id, and port number Iields that 
contain either a wild card or a RangeId. Thus, we can 
concatenate the required number of bits from each filter 
to construct a hash key for that lilter. We store all filters 
mapped to T in a hash table Hashtable( A probe in 
a tuple T involves concatenating the required number of 
bits from the packet as specified by T (after converting 
port numbers to Rangelds), and then doing a hash in 
Hushtable( 
Thus, given a packet P, we can linearly probe all the 
tuples in the t,uple set, and determine the least cost filter 
matching P. While this is a very naive search strategy, 
the search cost is proportional to m, the number of dis￾tinct tuples. On the other hand, the current practice 
of performing a linear search through t.he filter database 
has cost N, which tends to be much larger than m. 
We ran tests on 4 industrial firewall dat.abases which 
we refer to generically as Fwal-1 to Fwal-4. We found 
that while the number of filters ranged from 68 to 278, 
the number of tuples ranged from 15 to 41. The following 
table shows this empirical statistic. 
Table 1: Four Firewall databases. 
Thus, even without any additional improvementa; tu￾ple space search seems bett.er than linear search. Fur￾thermore, since each filter belongs to a unique tuple, the 
update cost (inserting or deleting a filter) is small; just 
one memory access for a hash, assuming a perfect hash 
function that is chosen to avoid hash collisions. 
6 Tuple Pruning Algorithm 
We now describe our first. heuristic for improving tuple 
space search, which is not only very simple but also gives 
the best search and update performance of all our heuris￾tics. 
The main motivation behind this new heuristic, called 
Tuple Pruning, is that in real filter databases there seem 
to be very few prefixes of a given address. For exam￾ple, when we examined the Mae-East prefix database [7], 
we found that no address U has more than 6 match￾ing prefixes. Suppose this were true. If we consider 
Destination-Source filters, then naive Tuple Search may 
require searching 32 x 32 = 1024 (possible combinations 
of pairs of lengths for the destination and source prefix 
lengths) tuples. However, if both destination and source 
prefixes are taken from Mae-East, then if we first fmd the 
longest destination match and the longest source match, 
there are only at most 6 x 6 = 36 possible tuples that are 
compatible with the individual destinat,ion and source 
matches. Thus we have reduced the number of tuples 
to be searched from 1024 to 36 at the cost of an extra 
dest.ination and source prefix match. 
In essence, tuple pruning seeks to generalize this em￾pirical observation to arbitrary filters by first doing indi￾vidual longest. prefix (or range matches) in each dimen￾sion and then searching only the tuples that are compati￾ble with the individual matches. Tuple pruning will ben￾efit if the reduction in the tuple space afforded by prun￾ing offsets the ext,ra individual prefix (or range) matches 
on each field. In some sense, this is similar to the Lu￾cent [B] scheme except for the following major difference: 
while both schemes first do independent matches in each 
dimension, the Lucent scheme seasches t.hrough filters 
that are compatible with the individual matches while 
we search through tuples that are compat.ible with in￾dividual matches. Since, as we have said earlier, the 
number of tuplcs grows much slower than the number of 
tuples, we expect tuplc pruning to scale better than the 
Lucent scheme. 
To set up Tuple Pruning, for each destination prefix 
D in the database, we compute a tuple list (or bitmap) 
containing the names of tuples that have a filter with 
destination equal to D or a prefix of D. Similarly, for 
each source address S in the dat,abase, we compute a list 
containing the names of tuples that have a filter with 
source equal to S or a prefix of S. We can do this for the 
protocol and port number fields as well, but our imple￾mentation results seem to indicate that (at least for the 
databases we had) the results do not improve much by 
using additional fields. 
For instance, suppose 11 = lOlO*. Suppose all the 
filters whose destination is a prefix of D belong to tuples 
[l, 41, [l, 11, and [2,3]. Then, the tuple list of D contains 
these 3 tuples. Similarly suppose S = OOlO*. Suppose all 
t,he filters whose source is a prefix of S belong to tuples 
[3,41, [I, 11, and P, 51. 
Now, our search algorithm works as follows. Given 
a packet header P, we first compute the longest match￾ing prefix PD of the destination address P[l], and the 
longest matching prefix PS of the source address P[i]. 
We now take the tuples lists st.ored with PD and SD, and 
find their common intersection. For instance, if PD = D 
and PS = S in the above example, the intersection list 
only includes tuple [1, 11. We now probe the tuples in 
this intersected list. 
The updat,e algorilhm is also quite simple. First, we 
refine our description of the search process to describe 
how the the t.uple list associated with a prefix D is com￾puted. Rather than store t,he tuple list of D and all its 
prefixes with D, we store only the tuples associated with 
D. We can obtain the tuple lists associated with prefixes 
of D if we do a trie search for D because such a search 
139 
must necessarily encounter all the prefixes of D. 
When a new filter is added into the database, we add 
its destination and source prefixes (say D and S) to the 
destination and source tries. Next, for each prefix we add 
(for example D), we maintain an augmented list of tuples 
corresponding to filters that contain D. The augmented 
list is a list of tuples (as before) but each tuple T also 
contams a reference count of the number of distinct filters 
that have destination D and map to T. The reference 
count is useful for deletion of filters which decrements the 
reference count. in an analogous way. When the reference 
count reaches zero, the corresponding tuple is removed 
from the tuple list of the associated prefix. Note that 
the augmented tuple lists containing count,s need not be 
part of the search data structure but must be maintained 
by the update process. Thus the update time requires 2 
best matching prefix operations plus a constant time to 
update the augmented tuple lists for each of three fields. 
Table 2 shows the effect of Tuple Space Pruning on 
our fiewall databases. We calculated the size of the 
worst case set of pruned tuples for all possible combina￾tions of source, destination, and protocol matches. This 
pruned size is shown in the Pruned Tuplcs column. Note 
that this simple heuristic prunes the tuple space by a 
factor of at least three. We tried pruning on port fields 
as well, but found that we did not get better numbers. 
However, when we used the two port fields instead of the 
address fields for pruning, we obtained similar results. 
Thus for the small databases we examined pruning based 
either on destination and source addresses or destination 
and source ports produced the best results. However, for 
larger databases, we expect that pruning on all fields will 
be beneficial. 
Database 1 Size 1 Tuples 1 Pruned Tuples 
Fwal-1 I 278 I 41 I 11 
Table 2: The worst-case number of hash probes produced 
by the Pruned Tuple Search Method. 
6.1 Experiments with random filters 
We believe that as the filter database grows larger the 
additive cost of doing a separate best matching prefix 
on the destination and source fields will remain a con￾stant, but the relative gains of Pruned Tuple search will 
remain. Since there does not seem to be any publicly 
available large filter database, we experimented by cre￾ating random filters to see how well the tuple pruning 
algorithm scales. We generated N source-destination fil￾ters, where the source and the destination prefix of each 
filter was chosen uniformly at random from the MaeEast 
database [7]. We tried pruning on the destination field, 
the source field, and then on both the fields. We present 
our experimental results in Table 3. When the num￾ber of filters is large (> lOOOO), we could not strictly 
test, the worst case pruned tuple size for all possible 
crossproducts. Thus for those two cases, we did a sta￾tistical sampling test by considering a million randomly 
chosen crossproducts, and finding the worst case size of 
the pruned tuples across all the random samples. 
Size 1 Tuples I Dest- I Src- I Both- 1 
1 Pruned 1 Pruned 1 Pruned 
1000 I 53 I 1 I 1 I 1 
Table 3: Number of tuples found in a randomly gener￾ated filter database, and the effect of pruning. Prefixes 
were randomly chosen from the MaeEast database. 
While this test does not in any way guarantee that 
Tuple Pruning will scale as well for large databases in 
practice, reducing the tuple space from say 104 (naive 
tuple search) to 2 (pruned tuple) at the cost of say 8 
more memory accesses, does suggest good scaling behav￾ior. More importantly, the Pruned Tuple Search algo￾rithm has a very fast update time unlike the Balancing 
Heuristic algorithm we describe later (and all the other 
techniques in the literature). Thus Pruned Tuple Search 
is the only scheme we know of that provides fast search 
times and yet can be used for applications like Stateful 
Packet filters and RSVP filters. 
7 lniproved Tuple Space Search via Precomputation 
Naive Tuple search can have a large search time; pruned 
tuple search can improve the search time dramatically 
but it depends on assumptions on the structure of exist￾ing databases. Can we improve the performance of tuple 
search and provide some worst case guarantees without 
making such assumptions? The remainder of this paper 
is devoted to answering this question. In essence, our re￾sults will show that we can do fairly well for the case of 
two dimensional filters by using precomputation (which 
increases filter update time), but the worst case improve￾ment. is only marginal for the general filter problem. 
The ideas described below can be considered to be 
a generalization of the ideas in [14] for finding longest 
matching prefix. For example, simple tuple space search 
will take O(W) time for finding the longest matching 
prefix of a W length destination address field, but the 
techniques of markers and precomputation can be used 
to reduce the search time to O(log W) at the cost of 
slower insertion times [la]. We will show that for two￾dimensional filters we can reduce the search time from 
W* memory accesses to 2W accesses using similar tech￾niques. We will also describe lower bounds to show thal 
this algorithm is optimal. 
We now consider such strategies for reducing the search 
space. The main idea is that a probe into a tuple Ti can 
be used to eliminate a subset of the tuple space. In par￾ticular, if a probe succeeds: then we can eliminate all the 
t.uples that are coordinate-wise shorter than T, , because 
we can precompute the best matching filter from those 
140 
tuples and store the answers with filters in Ti. Similarly, 
if a probe fails in Ti, then we can eliminate all the tuples 
that are coordinate-wise longer than Ti, because each fil￾ter in those t,uples can leave a marker filter in T,. This 
is a time-space tradeoff, because markers require addi￾tional memory, but the search cost can be reduced. We 
now describe the marker and precomputation ideas in 
more detail. 
7.1 Markers and Precomputation 
Consider a tuple Ti = [1i, 12,. . . , ZK]. We can partition 
the remaining tuple space into 3 disjoint parts, Short(T,), 
Long(Ti) and IC(T,) (where IC stands for incompara￾ble). The set Short(Ti) contains all those tuples whose 
length vector is coordinate-wise shorter than T,. That 
is, a tuple Tj = [hi, hz, . . . , hrc] belongs to set Sho,t(Ti) 
ifandonlyifh;<1;,foralli=l,:! ,..., K,andT,#T,. 
Similarly, Long(T;) contains all those tuples whose 
length vector is coordinate-wise longer than Ti. Specifi￾cally, a tuple T, = [hl , hz, . . . , /rlc] belongs to set Long(Ti) 
if and only if lai 2 I,, for all i = 1,2,. , Ii*, and T, # Ti. 
A tuple Tj, where T, # T;, that is neither in Short(K) 
nor in Long(T,) belongs to the incomparable set IC(T;). 
Figure 1 shows this partitioning of the tuple space into 
the three sets. 
As an example, consider the tuple T = [2,3,2]. Then 
tuple [l, 1, l] belongs to Short(T); the tuple [5,3,3] be￾longs to Long(T); and tuple [l, 4, l] belongs to IC(T). 
Fail (Ti) 
Succ (Ti) 6 ____________________-. 
Figure 1: Figure showing the three sets obtained when 
a probe is done at tuple T,. The three sets are mutually 
exclusive and every tuple other than ,T, falls into one 
of the three sets. The sets Fail(Ti) and SUcc(Ti) are 
defined later. 
Since each tuple in Short(T) is less specific than T on 
all fields, it is possible to precompute the best matching 
filter information for the set Short(T) and store it with 
the filters in T. Specifically, let F be a filter in T. \Ve 
can precompute the best filter matching F in the set of 
filters than belong to Short(T). Thus, if we get a match 
with filter F during the probe in II’, we no longer need 
to search Short(T). In other words, if the probe at T 
returns a match, we can restrict our search to the tuples 
in IC(T) and Long(T). Their union is the set Succ(T) 
illustrated in Figure 1. 
Similarly, we have each filter F in Long(T) leave a 
marker, which is the filter obtained by using only 1, bits 
of the e’th field of F, where [1i, 12,. . . , II;] is the length 
vector for T. Since each filter in Long(T) has at least 1; 
bits specified in field i, this is possible. (As an example, a 
filter F = (lOlO*, llO*) of tuple [4,3] leaves the marker 
F’ = (lo*, II*) in the tuple [2,2].) Now, if we probe T 
and do not get a match, we can easily eliminate the set 
Long(T)-if any filter in that set matched the packet 
header, its marker entry in T would have produced a 
match. Thus, when the probe in T fails, we can restrict 
our search to the tuples in IC(T) and Short(T). Their 
union is the set Fail(T) illustrated in Figure 1. 
While this general strat,egy seems promising, we need 
to find a specific instantiation of the strategy (which 
would specify the sequence of tuples to be probed) that 
can provide an improvement in the worst-case. We start 
by showing a specific search st,rategy that works well for 
the two-dimensional case. Later we show some lower 
bound results that shed some light on the difficulty of the 
general problem. Finally, we return to the general prob￾lem and show a heuristic search strategy that attempts 
to compute an optimal probe sequence for a given filter 
database, using precomputation and markers. 
8 Rectangle Search: An Optimal Algorithm for 2-D 
Filters 
One can argue that two-dimensional filters (especially, 
destination-source filters) are interesting in their own 
right for multicast forwarding or virtual private networks 
(VPNs) [8]. A very simple application of a two dimen￾sional filter database (suggested by Jon Turner) is a net￾work monitor that computes traffic statistics between all 
source and destination subnetworks. In this section, we 
will rest.rict ourselves to two dimensional filters. 
I Filters 
F~101*,1110* 
Z=l',l' 
Markcrs 
F4 lOl*, 111’ 
F3 lOl*, 11’ 
F2 101*,1* 
Fl 101’. * 
Zl 1*.* 
Figure 2: Illustration of markers and precomputation 
Several efficient packet classification algorithms exist 
for the t.wo-dimensional problem, such as range matching 
based on fractional cascading [8] or grid-of-tries for prefix 
matching [13]. Is there a similar efficient algorithm for 
the 2D case in the tuple space model? We show below 
that the answer is yes. 
We give a simple algorithm than computes the best 
matching filter in a I%’ x W tuple space using 2W - 1 
probes, which is optimal in view of a lower bound we 
show later. The algorithm uses markers and precompu￾tation to eliminate subsets of the tuple space after each 
probe. We call this algorithm Rectangle Search, and 
it uses a different marker and probing strategy than the 
heuristics of Section 7.1. We now describe the algorithm. 
A filter leaves a marker at all the tuples to its left in 
its row. So, a filter F that belongs in tuple [& j] leaves a 
141 
marker in tuples [i, j-l], [i, j-21, . . . , [i, 11. Each fiber (or 
marker) also prccomputes the least cost filter mat,ching 
it. from among the tuples above it in its column. That 
is, a filter (or marker) in tuple [i,j] precomput.es the 
least cost filter matching it from the tuples [i - 1, j], [; - 
2, j], . . . , [l, jl. 
Figure 2 shows an examnle of orecomnutation and 1 
markers, using two filters F and Z. The marker Fz pre￾computes the best matching filter among the entries in 
the column above it, which in this example is 2. 
Figure 3: If a probe in tuple T results in a match, then 
the entries in the column above T are eliminated; if the 
probe results in no match, then the entries in the row 
to the right of T are eliminated. In both cases, the next 
probe is done in tuple T’. 
Given these markers and precomputation, we can now 
describe a search strategy that outputs the best matching 
filter after 2 W - 1 worst-case probes. We start by probing 
the lower-left tuple, namely, [W, I]. At each tuple, if the 
probe returns a match, we move to the next. tuple in the 
same row. If the search returns no match, we move up 
one row, in the same column. See Figure 3. 
When we get a match in a tuple, the matching fil￾tcr’s precomputed informat,ion makes searching the tu￾ples above it in its column unnecessary, and so we can 
eliminate them from the search. This allows us to move 
to the next column. If we fail to get a match, t.he marker 
rule tells us that. there is no filter to t,he right of the cur￾rent tuple; because otherwise t.hat fiber’s marker would 
have produced a match. In this case we can eliminate 
the tuples to the right of the current tuple, and move to 
the row above. Thus, each probe eliminates a row or a 
column. The search terminates when the we reach the 
rightmost column or the first row. Since there are l%’ 
rows and W columns, the number of probes needed is at 
most 2W - 1. 
8.1 Time-Space Tradeoffs and Generalized Rectangle 
Search 
Rectangle search requires O(NW) memory, since each 
filter leaves at most W markers. This performance is 
comparable to other O(log N) search algorithms using 
O(Nlog N) memory, since W NN log N. We can trade￾off memory for speed by using fewer markers. For a 
filter in column i, we can leave markers in every col￾umn to the left of i that is a multiple of some k, and 
in each column after t.he largest multiple of k smaller 
than i. This increases the required number of hashes in 
the worst case from 2W to 3W, but. reduces the mem￾ory to 0( NW/k). Choosing k = m gives us an O(W) 
worst-case algorithm for the 2dimensional filters with 
0( Nm)memory. 
Rectangle Search is designed for square tuple spaces. 
What. can bc done for non-square tuple spaces? If we 
have a R x C rectangular tuple space, then Rect.angle 
Search gives a worst-case bound of R + C - 1. However, 
notice that if R << C, then binary search in each row 
can achieve O(IZ log C), which may be much better. We 
can extend our Rectangle Search with a new idea, called 
doubling search, which allows us to search a R x C tu￾ples space in 0( Rlog(C/R)) probes. We do not describe 
doubling search here for lack of space. The bound for 
doubling search nicely interpolates the optimal bounds 
at the two extremes: O(log W) for the linear tuple space, 
and O(W) for the square tuple space. 
In table 1 we compare several 2dimensional lookup 
algorithms and their worst case search and memory com￾plexities. 
I Scheme 
Linear Search 
Rinary search 
Grid-of-tries 
I Memory I Search 1 
O(N)- O(N) 
O(N) O(W + log N) 
OtNWI O(W) 
Rectangle Search 1 O(N&) 1 oiwj 1 
Table 4: Complexity of 2-dimensional lookup algorithms. 
9 Some Lower Bounds 
The preceding sections have described several algorithms 
for the best mabching filter problem in the tuple space 
search paradigm. The attractiveness of our algorithms 
is based on the premise that the tuple space of practical 
databases is quit.e sparse. Several researchers have also 
investigated packet classification algorithms with good 
worst.-case guarantees [8, 131. Although some nice re￾sults are known for the two-dimensional filters, none of 
the known algorithms scale well to multi-dimensional fll￾t,ers. As the dimension increases, either the search t.ime 
degrades quickly, or the memory requirement grows ex￾ponentially. 
In this section, we argue that the packet classification 
problem indeed suffers from the curse of dimensionality, 
and formally establish a lower bound on the number of 
hash probes needed to find the lowest. cost matching fil￾ter in the tuple space model. Let us first consider the 
simple case of 2-dimensional (source-destination pair) fil￾ters. What is the best possible bound for searching the 
W x W tuple space? Is O(log W) possible? We estab￾lish a lower bound showing that at, least 2W - 1 probes 
must be made in the worst case. In fact, we prove a 
142 
more general result: if the tuple space has R rows and C 
columns, then roughly R log( 1+ g) probes are necessary 
in the worst case, where C 2 R. This shows that our 
rectangle search algorithm is essentially optimal in the 
tuple search paradigm. 
We will use the “decision tree model” of computation 
for our lower bound argument. The decision tree model 
is an abstraction of branching programs, in which each 
internal node represents a decision, which has a binary 
outcome. (For instance, in sorting the node corresponds 
to a comparison test between two e1ement.s. In our case, 
the test involves checking whether the header of a packet 
matches any of the fIlt.ers in a tuple, which takes one 
hashed memory access.) The algorithm starts by per￾forming the test at the root of the decision tree. Based 
on the outcome, the program branches to one of the chil￾dren, and cont,inues until a leaf node is reached, at which 
point. the algorithm produces its output. The execution 
of the algorithm on a specific instance corresponds to 
a path in the decision tree from the root to some leaf. 
The worst-case running time of the algorithm, therefore, 
corresponds to the height of the decision tree. Showing 
a lower bound of L on a problem’s complexity requires 
one to show that there are at least zL “configurations” of 
the input where no two configurations correspond to the 
same leaf node in the decision tree. We show that the 
problem of searchin 
has roughly (v ’ 
a R x C tuple space, where C 1 R, 
) distinct configurations, which will 
lead to a R(Rlog g) lower bound. 
Consider a fixed packet header with say source ad￾dress S and destination address D. Our argument est.ab￾lishes a lower bound for the search cost of any decision 
tree algorithm for this fixed header (S, D) over all pos￾sible input instances, where the input is a set of filters. 
Given an instance of the titer database, imagine color￾ing all those tuples red that contain a filter matching 
the header (S, D). Starting from the lower-left corner, 
we can draw a unique leftmost staircase that contains 
all the red tuples on or above it. Figure 4 (i) illustrates 
this concept. In this manner, each instance of t.he filter 
database can be associated with a staircase. The key 
point is that, for each instance, none of the tuples below 
the staircase contain a filter matching the pair (S, D). 
We now show that if two instances of the filter database 
have distinct staircases, then the decision tree algorithm 
must follow different paths on these inputs. Indeed, sup￾pose we have two instances corresponding to staiic&ses 
21 and ZZ. Then, there is a tuple T that lies on or 
above one staircase but below the other. Without loss of 
generality, suppose T lies above ZI , and below Zz. (See 
Figure 4 (ii), where the staircase Z1 is shown in dashed 
lines.) Clearly, if the algorithm probes the tuple T dur￾ing its course, then the search path for Z1 and Zz would 
diverge at that node-since we get a match for Z1 and 
no match for Zz. Thus, the search algorithm must not 
have probed T. 
A common decision path for Z1 and Zz means that 
the algorithm probes the same set of tuples, makes the 
same branching decision at each node, and therefore out￾puts the same tuple as its answer. A simple consequence 
of this observation is that on this particular search path, 
the algorithm cannot output tuple T as t.he best match￾ing filter tuple-since the search path is common to both 
fi) (ii) 
Figure 4: (i) A staircase. The tuples that match t,he 
header (S, D) are shown with solid disks. The staircase 
is shown in thick lines. (ii) Shows two staircases. Tuple 
T lies above the staircase shown with dashed lines and 
below the one with solid lines. 
21 or Zg, the output tuple must be a red tuple for both. 
But then if we simply put the least cost filter in T for 
a Z1 instance, the algorithm’s output will be incorrect. 
Thus, every staircase must correspond to a distinct leaf 
node in the decision tree. 
Next, we establish a lower bound on the number of 
distinct staircases. This requires a simple combinatorial 
argument. If we have a grid of dimensions R x C, where 
C 1 R 2 3, then the number of staircases connecting 
t.he leftmost and rightmost corner is at least 
(C;R;l) 
One simple way to see this is as follows: each staircase 
can be uniquely identified by the positions of the vertical 
(unit) steps. There are R - 1 are vertical unit steps 
needed to go from bottom row to the top row. Now, 
we are ready to prove our lower bound. The worst-case 
search cost of the algorithm is at least as large as the 
height of the decision tree. If a binary tree has A4 leaves, 
it has height at least log, M. Thus, the height H of the 
decision tree has the following lower bound: 
H 1 log (‘+Rf;l) 
1 1% C+R-1 R-’ 
R-l > 
1 (R-1)10&+1) 
= fI(Rlog(1-t;)). 
This completes our proof of the lower bound for the 
two-dimensional tuple space. While the technique pre￾sented above is quite general, it does not yield the best 
possible lower bounds. For instance, when the tuple 
space is a W x W square, the proof above gives a lower 
bound of about W - 1, though it is possible to obtain 
a tighter bound of 3W - 1 using an adversary based ar￾gument, which we now present. However, the staircase 
lower bound provides a bound for a rectangular tuple 
space. 
143 
9.1 An adversary-based lower bound 
Consider the following set of tuples: 
Ml = {(11,12) ( 11 + 12 = W}. 
Since 0 < II, 12 5 W, there are precisely W tuples in MI, 
namely, (0, W), (1, L%’ - l), . . . , (W, 0). These tuples in 
fact correspond to the diagonal entries of the square tuple 
space. Using the definition of Section 7.1, the tuples of 
Ml are pairwise incomparable. That is, if T = (11, 12) 
and T’ = (1;,1;), where 11 + 12 = 1; + 1; = W, then 
11 < 2; implies 22 > 21. Thus, probing one tuple does 
not. yield information about the other-neither markers 
or precomputation help, since each tuple has a longer 
coordinate than the other. Similarly, we can define a 
second set of tuples 
hf2 = {(&,12) 1 I1 +/2 = w+ l}, 
which corresponds to the tuples along the second diago￾nal of the tuple space. The set M2 has precisely W - 1 
tuples. See Figure 5 for illustration. 
Main 
Diagonal 
Second 
Diagonal 
Figure 5: Adversary lower bound. 
Now, consider an arbitrary packet header (S, D). We 
create W filters, one per tuple of MI. Specifically, the fil￾ter F(i, j) corresponding to the tuple (i, j) is constructed 
by taking the i-bit prefix of S and j-bit prefix of D. For 
instance, assuming W = 3, S = 101, and D = 011, 
the filter for tuple (1,2) is (l*,Ol*). Each of these fil￾ters may be assigned an arbitrary cost. The adversary’s 
strategy is as follows: if the algorithm probes a tuple 
in MI, the adversary returns the matching filter; if the 
algorithm probes any other tuple, the adversary returns 
“no match.” Notice that this strategy is consistent-the 
matches along the main diagonal only eliminate the up￾per triangular tuple space (precomputation), while the 
lack of matches along the second diagonal only elimi￾nates the lower triangular tuple space (markers). 
We claim t,hat any tuple space algorithm must probe 
at least 21/V - 1 tuples to correctly find the best matching 
filter for the header (S, D). Clearly, the algorithm must 
probe the W tuples of MI-ot.herwise, since we can arbi￾trarily choose the filter costs, the filter in the unprobed 
tuple can be made the cheapest filter, thus foiling the 
algorithm. The key observation here is that. since the tu￾ples of Ml are pairwise incomparable, probing one does 
not reveal any information about any other tuple in MI. 
In addition to the tuples of Ml, the algorithm must 
also probe all tuples in M2. The adversary returns “no 
mat.ch” on any tuple of M2 probed. But if the algorithm 
fails to probe a tuple, say, (i, j + l), where i + j + 1 = 
W + 1, then we can put a least cost filter matching (S, D) 
in it, and foil the algorithm. Thus, in order for the algo￾rithm to correctly determine the least cost filter matching 
(S, D), at least 2W - 1 probes must be made. 
Note that this argument cannot be extended to a 
third diagonal-probing a cell of the middle diagonal will 
either eliminate some entries on the diagonal above (if a 
match is found) or it would eliminate some entries on the 
lower diagonal (if no match is found). 
9.2 Extension to multi-dimensional filters 
The adversary lower bound argument can be extended 
to &dimensional filters, as follows. Consider the set of 
tuples 
M = {(11,12 :...,lk) 1 11+z2+...+zk=w}. 
Clearly, the tuples in set M are pairwise incomparable￾if T is shorter than T’ in some dimension, then T must be 
longer than T’ in some other dimension to keep the sum 
constant. Now, given a packet header (HI, Hz,. . . , Hk), 
we can create a filter for tuple (11~12, . . . , lk) by taking the 
II-bit prefix of HI, the 22-bit prefix of HZ and so on. Each 
of these IMI filters match the packet, and since these fil￾ters are pairwise incomparable, an adversary can force 
the algorithm to search all I M( t.uples in order to find the 
least cost filter. So, how many tuples are in the set M? 
An easy inductive proof shows that ) MI 2 q. Thus, 
in the hashing model of tuple space search, packet classi￾fication among arbitrary filters requires at least w 
tuple probes in the worst case. 
10 Applying Markers and Precomputation for General 
Tuple Search 
So far, we have introduced the notion of using mark￾ers and precomputation to improve worst case search 
time in tuple search (Section 7), we have shown an opti￾mal two-dimensional algorithm (Section S), and we have 
shown lower bounds showing that markers and precom￾putation cannot improve the worst case significantly for 
general filter databases (Section 9). However, just as Tu￾ple Pruning (Section 6) can improve the performance of 
naive tuple search for specific databases, the question still 
remains: can we find an optimal search strategy using 
markers and precomputation for specific databases. We 
already know how to do this for 2D databases, but this 
certainly does not apply to t,he real firewall databases 
Fwal-1 to Fwal-4 that we used earlier. We now address 
this question. 
10.1 A Dynamic Programming Algorithm 
Before reading on, the reader may wish to review the 
general strategy in Section 7.1 and the terminology used 
there. 
Given a tuple space TS, we wish to determine the op￾timal sequence of probes for computing the best match￾ing filt.er, using markers and precomput,ation to eliminate 
tuples after each probe. This essentially corresponds to 
constructing a decision tree in which each node is labeled 
with a probe, and the algorithm branches to one or the 
other subtree based on the probe outcome. Suppose we 
134 
probe tuple Ti in the first step; then a match in T; results 
in searching the set Succ(T;), while a fail in T, results 
in searching Fail(X). The optimal cost of searching the 
tuple space TS can then be written recursively as follows: 
Opt(TS) = 1 + F( max{Opt(Succ(T,)), Opt(Fail(Ti))}. 
The recurrence is based on the fact that after the 
probe done in the first step, one of the two tuple sub￾sets, Succ(Ti) or Fail(Ti), needs to be solved optimally, 
and the best first probe is the one that minimizes the 
maximum of the two costs. 
Unfortunately, since the subsets Succ(Ti) and FaiZ(Ti) 
are not disjoint, and can have significant overlap, com￾puting Opt(TS) takes exponential time in the worst case. 
Specifically, in the worst-case, the recurrence for comput￾ing Opt(TS) can be written as 
f(m) = 2mf(m - 1) + m2, 
where m is the number of tuples in the tuple space and 
m2 is the complexity of forming the sets Succ(Ti) and 
Fd(T;). This gives f(m) _> 2”, which is exponential. 
0 
Figure 6: Each level has at most m tuples, and there 
are at most m levels. Processing each level takes O(m2) 
time, for a total of O(m3). 
10.3 Implementation and Measurements 
We demonstrate the performance of our heuristic on the 
4 industrial fnewall databases shown in Table 1. In Ta￾ble 5, we show for each of the fiewall databases the 
worst-case number of probes, as found by the heuristic. 
10.2 Tuple Search using a Balancing Heuristic 
Given the prohibitive complexity of computing Opt(TS), 
we use some heuristic algorithms. Rather than com￾puting Opt(Succ(Ti)) and Opt(Fail(T,)) recursively, we 
simply use some estimates on their costs. One simple 
estimate is the number of tuples in the set. That is, as 
the first probe we pick the tuple Ti that minimizes the 
larger of JSucc(T,)j and jFail(Ti)l, and t,hen recursively 
work on the sets Succ(Ti) and FaiZ(Ti) using the same 
heuristic. Unfortunately, even this heuristic ends up be￾ing exponential! Indeed, the recurrence for this heuristic 
is f(m) = 2f(m - 1) + m2, which is better than the pre￾vious one, but still leads bo exponential time. The main 
difficulty remains that the total size of the two subprob￾lems, Succ(Ti) and Foil(Ti), is still large. 
In our final heuristic, we treat the common elements 
of the two sets, namely, Succ(Ti) f~ Fail(Ti), separately. 
In other words, when we choose a tuple Ti to probe, 
we divide the set into three parts as shown in Figure 6. 
We then use a simple balancing heuristic for the cost 
function: 
Cost(Ti) = JIC(Ti)I + max {JShort(T;)I, /Long(T; 
The t.uple Ti with smallest Cost is used as the first probe. 
We then recursively solve the problem for the three sub￾sets ZC(T,), Short(Ti) and Long(Ti). This heuristic runs 
in polynomial time, and can be analyzed as follows. 
Initially, we start with m tuples. (See Figure 6.) Af￾ter O(m’) computation, we decide the tuple that will be 
probed first in the search sequence. This leaves us with 
m - 1 tuples in the next level. We do O(m2) computa￾tion at each level to choose the tuples to be probed at the 
next level. The maximum number of levels is m, since 
we can choose each of the tuples only once. This gives 
us a total time complexity of O(m3). 
Table 5: Number of probes is the worst-case search 
length found by the balancing heuristic. The time is 
the time needed by the heuristic in seconds to compute 
the probe decision tree. As the table shows, the heuristic 
seems to reduce the number of probes by about 40%. 
When comparing the results of Pruned Tuple Search 
(Table 2) to that of the Balancing Heuristic (Table 5), 
we may at first conclude that the Balancing Heuristic 
is much worse (for example 23 tuples versus 11 for the 
largest database). However, we have not accounted for 
the cost of computing the best matching prefix on the 
Destination and Source fields in Pruned Tuple Search. 
Using good algorithms, this can add 4 hashes for Des￾tination, 4 for Source, and 1 for Protocol. This would 
yield more comparable numbers. 
11 Conclusion 
In this paper, we have presented a new packet classifica￾tion algorithm that we call tuple space search. Simple 
tuple space algorithm searches t.hrough the field length 
combinations found in the filter set. It is motivated 
by the observation that the number of tuples in real 
databases is much smaller t.han the number of filters. 
Our experimental results were limited by the moderately 
size databases we had access to, the largest of which had 
278 filters and 41 tuples. However, we believe that even 
for very large filter database (say a million filters), the 
tuple space is unlikely to grow beyond a few hundred. 
This is because most databases use only a few prefix 
145 
lengths corresponding to Class A, Class B, and Class C 
addresses and a small number of port ranges. 
Even if the total tuple space were to grow into the 
thousands (using 32 possible destination and source pre￾fix lengths), we argue that pruned tuple search will pro￾duce a much smaller set of pruned tuples. We have ex￾amined the Mae-East IP prefix database and have found 
that, no prefix D has more than 6 prefixes that. are pre￾fixes of D in the worst case. Thus even the number of 
D-S tuples is probably bounded by 36 instead of 32*32 = 
1024. Because our empirical evidence shows a substan￾tial reduction in the pruned set of tuplcs, we expect this 
behavior to be valid for larger databases. Pruned Tuple 
Search is also t.he only scheme we know of that has fast 
update times. It, seems appropriate for software firewall 
implementations that require dynamic updates. 
Despite the apparent utility of Pruned Tuple Search, 
Pruned Tuple search does not guarantee a good worst 
case search time for arbitrary databases. Thus we spent 
a significant portion of this paper investigating whether 
techniques based on precomputation and markers could 
improve the worst case search time of tuple space search 
at the cost of increased update times. As shown in [14], 
for the IP address lookup, which can be thought of as the 
one-dimensional packet classification problem, markers 
and precomputation improve the worst case search cost 
from W to log W. Our paper shows that similar tech￾niques improve the search cost from W2 to 2W - 1 in the 
two-dimensional packet classification. Our lower bounds 
demonstrate that for I~dimensional filters, where I< > 
2, the search time is O(W”-‘). Thus there is a point 
of diminishing returns beyond two dimensions where the 
use of markers and precomputation does not improve 
worst-case search times significantly. 
While the lower bounds preclude the ability to fmd 
an algorithm based on tuple search that has a fast search 
time on all databases, it does not preclude algorithms 
that work well on specific databases. Thus to complete 
the investigation, we also examined a Balancing Heuristic 
for generalized tuple search based on markers and pre￾computation. Our experimental results for the Balanc￾ing Heuristic were not encouraging, and the Balancing 
Heuristic does not seem to scale as well to large databases 
as does the Tuple Pruning Heuristic. However, the Bal￾ancing Heuristic may out.perform Tuple Pruning search if 
the tuple space becomes sufficiently dense to allow more 
tuples to be eliminated by precomputation and markers. 
Another alternative for a dense tuple space is to break 
up the space int.o multiple rectangles, and perform Rect￾angle search. 
While our paper has emphasized software implemen￾tation, we note that tuple pruning search has a simple 
parallel implementation where each tuple can be probed 
in parallel. Thus, in conclusion, we believe that tuple 
pruning search is simple and scalable, has fast update 
times and has a simple parallel implementation. Finally, 
rectangle search provides an optimal algorithm for two￾dimensional filters. 
Acknowledgement 
We thank Jonathan Turner for an observation that led 
us to the tuple pruning algorithm. Marcel Waldvogel in￾dependently invented a specialized form of tuple search, 
called line search. We also thank Paul Vixie for providing 
us with firewall databases. 
References 
PI 
PI 
[31 
[41 
PI 
161 
[71 
PI 
PI 
PO1 
Pll 
WI 
D31 
P41 
M. L. Bailey, B. Gopal, M. Pagels, L. L. Peter￾son, and P. Sarkar. PATHFINDER: A pattern￾based packet classifier. Proc. of the First Sympo￾sium on Operating Systems Design and Implemen￾tation, 1994. 
J. Boyle. Internet Draft: RSVP Extensions for 
CIDR aggregated data flows. Internic, 1997. 
W. Cheswick and S. Bellovin. Firewalls and Internet 
Securily. Addison-Wesley, 1995. 
D.B. Chapman and E.D. Zwicky. Building Internet 
Firewalls. O-Reilly & Associates, Inc., 1995. 
D. Decasper, Z. Dittia, G. Parulkar, and B. Plat￾tner. Router Plugins: A Software Architecture for 
Next Generation Routers. Proc. of ACM Sigcomm, 
1998. 
D. Engler and M. F. Kaashoek. DPF: Fast, Flex￾ible Message Demultiplexing using Dynamic Code 
Generation. Proc. of ACM Sigcomm, 1996. 
Merit Inc. IPMA Statistics. 
http://nic.merit.edu/ipma. 
T. V. Lakshman and D. Stidialis. High Speed 
Policy-based Packet Forwarding Using Efficient 
Multi-dimensional Range Matching. Proc. of ACM 
Sigcomm, 1998. 
G. Malan and F. .Jahanian. An Extensible Probe 
Architecture for Network Protocol Measurement. 
Proc. of ACM Sigcomm, 1998. 
S. McCanne and V. Jacobson. The BSD packet fil￾ter: A new architecture for user-level packet cap￾ture. USENIX Technical Conference Proceedings, 
1993. 
P. Newman, G. Minshall, and L. Huston. IP Switch￾ing and Gigabit Routers. IEEE Communications 
Magazine, 1997. 
C. Partridge. Locality and Route Caches. NSF 
WTorkshop on Internet Statistics Measurement and 
Analysis, 1996. 
V. Srinivasan, G. Varghese, S. Suri, and M. Wald￾Vogel. Fast Scalable Level Four Switching. Proc. of 
Sigcomm, 1998. 
M. Waldvogel, G. Varghese, J. Turner, and B. Plat￾tner. Scalable High Speed IP Routing Lookups. 
Proc. of Sigcomm, 1997. 
146 
